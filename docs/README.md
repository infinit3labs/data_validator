# Data Validator Documentation

Welcome to the Data Validator documentation! This multi-engine data validation framework helps you implement robust data quality checks using YAML configuration.

## ðŸ“š Documentation Index

### Getting Started
- **[Getting Started Guide](getting_started.md)** - Quick tutorial to get up and running
- **[CLI Usage](cli_usage.md)** - Command-line interface documentation
- **[Configuration Merging](config_merge.md)** - Environment variables and widget overrides

### Advanced Topics
- **[Databricks Integration](databricks_job.md)** - Databricks job deployment and management
- **[Pipeline State Management](pipeline_state.md)** - Idempotent pipeline operations

### Project Information
- **[Development Roadmap](roadmap.md)** - Future development plans and strategic vision
- **[Main README](../README.md)** - Project overview and quick start

## ðŸŽ¯ Quick Navigation

**New Users**: Start with the [Getting Started Guide](getting_started.md)  
**Databricks Users**: See [Databricks Integration](databricks_job.md)  
**Contributors**: Check our [Development Roadmap](roadmap.md)  
**Configuration Help**: Read [Configuration Merging](config_merge.md)  

## ðŸš€ Key Features

- **Multi-Engine Support**: PySpark, Databricks, DuckDB, Polars
- **YAML Configuration**: Intuitive, version-controlled validation rules
- **Databricks Native**: Unity Catalog, Delta Lake, DLT integration
- **DQX Integration**: Advanced data quality monitoring
- **Flexible Deployment**: CLI, jobs, or embedded in pipelines

---

For the complete project overview, see the [main README](../README.md).